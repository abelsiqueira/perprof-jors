\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenx}
\usepackage{cmap} 
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[top=3cm,bottom=3cm,outer=3.2cm,inner=3.2cm]{geometry}
\usepackage[backend=biber,sortcites=true,doi=false,url=false,firstinits=true,hyperref,maxbibnames=9,maxcitenames=3,sorting=nyt]{biblatex}

\addbibresource{refs.bib}
\usepackage{csquotes}
\usepackage{url}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{dsfont}       %Stroke Fonts for Number Sets

\def\Pset{\mathcal{P}}
\def\Sset{\mathcal{S}}

\begin{document}
\title{Perprof-py: a {P}ython package for performance profile}
\author{Raniere Gaia Costa da Silva\thanks{IMECC/Unicamp.} \and Abel Soares Siqueria\thanks{IMECC/Unicamp}  \and Luiz-Rafael Santos\thanks{IMECC/Unicamp}}
\date{Technical Report \\ Operational Research and Optimization Laboratory (LPOO) \\ IMECC/Unicamp \\ Campinas, Brazil \\ \today}
\maketitle

\begin{abstract}
Benchmarking optimization packages are very important in optimization field,
not only because it is one of the ways to compare solvers, but also to uncover
deficiencies that could be overlooked while one is developing a new solvers. During
benchmarking, one can obtain a large amount of  informations, like CPU time, number of functions
evaluations, number of iterations and much more. These informations, if presented
as tables, can be difficult to be analyzed, due, for instance, to large amount of data.
Therefore, researchers started testing and developing tools to better process and understand optimization benchmark 
data. One of the most widespread tools is \emph{ Performance Profile} graphics
proposed by \textcite{Dolan:2002du}. In this context, we implemented a free software that makes Performance Profile using data provided by user in a friendly manner. This software produces graphics in PDF using \LaTeX with PGF/TikZ~\cite{TikZ} and \texttt{pgfplots}~\cite{pgfplots} packages, in PNG using \texttt{matplotlib}~\cite{Hunter:2007}
and can also be easily extended to use with other plot library. The software is
implemented in Python3 with support for internationalization. %and is available on \url{https://github.com/lpoo/perprof-py}.

\textbf{Keywords:} Software benchmarking. Performance profile. Python.
\end{abstract}

\section*{(1) Overview}

\subsection*{Introduction} 

The Performance Profile of a solver is the cumulative distribution
    function of a performance metric, e.g., CPU time, number of functions
    evaluations, number of iterations, or others, that we will call \emph{cost}.

    Given a set $\Pset$ of problems and a set $\Sset$ of solvers, for each problem $p
    \in \Pset$ and solver $s \in \Sset$, we define $t_{ps}$ as the cost
    required to solve problem $p$ by solver $s$ and
    \begin{align*}
      r_{ps} = \frac{t_{ps}}{\min\{t_{ps}: s \in \Sset\}}
    \end{align*}
    as the performance ratio of solver $s$ for the problem $p$ when compared
    with the best performance by any solver on this problem.

    \textcite{Dolan:2002du} defines the probability for solver $s \in \Sset$ to solve one
    problem within a factor $\tau \in \mathds{R}$ of the best performance
    ratio as the function
    \begin{align*}
      \rho_s(\tau) = \frac{| \{p \in \Pset: r_{ps} \leq \tau\} |}{| \Pset |}
    \end{align*}
     For a given $\tau$, the best solver is the one with the highest
    value for $\rho_s(\tau)$.

    \subsection*{Motivation}
    To make easy the reproduce of the analyse of some data set, e.g. the benchmark of
    solvers, is important to have an open source tool that handle the production
    of plots. Beside this, for LaTeX users is possible to generate the plots
    with LaTeX and get a better typographic result other than include raster
    images.

\subsection*{Implementation and architecture}
How the software was implemented, with details of the architecture where relevant. Use of relevant UML diagrams may be appropriate. Please also describe any variants and associated implementation differences.

\subsection*{Input}
    Each solver to be used in the benchmark must have a file like:

    \begin{verbatim}
---
YAML information
---
Problem01 exit01 time01
Problem02 exit02 time02
    \end{verbatim}

    In the YAML information you can set the name of the solver, and some
    flags for perprof-py.
    Each line beyond that has 3 columns that mean, in order:
    \begin{itemize}
      \item The name of the problem;
      \item Exit flag;
      \item Elapsed time.
    \end{itemize}

\subsection*{Parsing process and output}

\subsection*{Quality control}
Detail the level of testing that has been carried out on the code (e.g. unit, functional, load etc.), and in which environments. 

\section*{(2) Availability}

\subsection*{Operating system}
Plataform independent.

\subsection*{Programming language}
python3.

\subsection*{Additional system requirements}
E.g. memory, disk space, processor, input devices, output devices.

\subsection*{Dependencies}
E.g. libraries, frameworks, incl. minimum version compatibility.

\subsection*{List of contributors}
Please list anyone who helped to create the software (who may also not be an author of this paper), including their roles and affiliations.

\subsection*{Software location:}
Archive  

\paragraph{Name:} perprof-py

\paragraph{Identifier:} \url{https://github.com/lpoo/perprof-py} 

\paragraph{Licence:} GPL (General Public License) Version 3

\paragraph{Date published:} 24/03/14

\paragraph{Publisher:} Name of the person who deposited the software

\paragraph{Date published:} 24/03/14

\paragraph{Code repository} 


\paragraph{Name:} GitHub

\paragraph{Identifier:} \url{https://github.com/lpoo/perprof-py} 

\paragraph{Licence:} GPL (General Public License) Version 3

\paragraph{Date published:} 24/03/14


Language

Language of repository, software and supporting files


\section*{(3) Reuse potential}

\printbibliography
\end{document}
